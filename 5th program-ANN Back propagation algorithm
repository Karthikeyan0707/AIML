
#Sigmoid Function

def sigmoid (x):
     return (1/(1 + np.exp(-x)))

#Derivative of Sigmoid Function

def derivatives_sigmoid(x):
     return x * (1 - x)

#Variable initialization

epoch=7000 #Setting training iterations
lr=0.1 #Setting learning rate
inputlayer_neurons = 2 
hiddenlayer_neurons = 3 
output_neurons = 1 


#weight and bias initialization
#with the help of numpy.random.uniform() method,we can get the random samples as numpy array by using this method.

wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons)) #2x3
bh=np.random.uniform(size=(1,hiddenlayer_neurons)) #1x3
wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons)) #1x1
bout=np.random.uniform(size=(1,output_neurons))

#Forward Propagation

for i in range(epoch):
    hinp=np.dot(X,wh)+bh
    hlayer_act = sigmoid(hinp)
    outinp=np.dot(hlayer_act,wout)+bout
    output = sigmoid(outinp)

#Backpropagation
    EO = y-output
    outgrad = derivatives_sigmoid(output)
    d_output = EO* outgrad
    EH = d_output.dot(wout.T)
    hiddengrad = derivatives_sigmoid(hlayer_act) 

#how much hidden layer wts contributed to error

    d_hiddenlayer = EH * hiddengrad
    wout += hlayer_act.T.dot(d_output) *lr

# dotproduct of nextlayererror and currentlayerop
    wh += X.T.dot(d_hiddenlayer) *lr


print("Input: \n" + str(X))
print("Actual Output: \n" + str(y))
print("Predicted Output: \n" ,output)
